# üéØ √âtape 1 ‚Äî D√©tection, Suivi et Extraction Squelettique (La Brique 1)

## üìã Summary (√Ä lire AVANT de commencer)

**Objectif** : Cr√©er le module de d√©tection unifi√© qui utilise **YOLOv8-Pose** pour extraire simultan√©ment les bo√Ætes englobantes, les IDs de suivi (ByteTrack), et les 17 points cl√©s du squelette de chaque personne d√©tect√©e.

**Dur√©e estim√©e** : 2-3 heures

**Pr√©requis** :
- ‚úÖ √âtape 0 enti√®rement valid√©e
- ‚úÖ Environnement virtuel activ√© avec toutes les biblioth√®ques
- ‚úÖ GPU CUDA fonctionnel

**Ce que vous aurez √† la fin** :
- ‚úÖ Mod√®le YOLOv8-Pose t√©l√©charg√© et test√©
- ‚úÖ Module `detector.py` fonctionnel avec suivi ByteTrack
- ‚úÖ Extraction des 17 keypoints par personne
- ‚úÖ Affichage temps r√©el : bo√Ætes + IDs + squelettes sur la vid√©o
- ‚úÖ Script de test valid√© sur une vid√©o de test

---

## üìù √âtapes D√©taill√©es

### 1.1 ‚Äî T√©l√©charger le Mod√®le YOLOv8-Pose

**Actions :**

```python
# Script de t√©l√©chargement : src/models/yolo/download_model.py
from ultralytics import YOLO

# T√©l√©charge automatiquement yolov8m-pose.pt (~50 Mo)
model = YOLO("yolov8m-pose.pt")
print(f"Mod√®le charg√© : {model.model_name}")
print(f"Type de t√¢che : {model.task}")

# V√©rification rapide sur une image de test
results = model.predict(
    source="https://ultralytics.com/images/bus.jpg",
    save=True,
    device=0  # GPU
)
print(f"Nombre de personnes d√©tect√©es : {len(results[0].boxes)}")
print(f"Keypoints shape : {results[0].keypoints.data.shape}")
```

> [!NOTE]
> Le mod√®le `yolov8m-pose.pt` (Medium) offre un bon compromis vitesse/pr√©cision. Pour plus de pr√©cision (mais plus lent), utiliser `yolov8l-pose.pt` (Large). Les 17 keypoints suivent le format COCO :
> 
> `0:nez, 1:≈ìil_gauche, 2:≈ìil_droit, 3:oreille_gauche, 4:oreille_droite, 5:√©paule_gauche, 6:√©paule_droite, 7:coude_gauche, 8:coude_droit, 9:poignet_gauche, 10:poignet_droit, 11:hanche_gauche, 12:hanche_droite, 13:genou_gauche, 14:genou_droit, 15:cheville_gauche, 16:cheville_droite`

**‚úÖ Crit√®re de validation 1.1** :
```python
# Le script doit afficher :
# - Type de t√¢che : pose
# - Nombre de personnes d√©tect√©es : ‚â•1
# - Keypoints shape : (N, 17, 3)  ‚Üê N personnes, 17 points, 3 valeurs (x, y, confidence)
```

---

### 1.2 ‚Äî Comprendre la Sortie de YOLOv8-Pose

Avant de coder le module, il faut bien comprendre la structure des r√©sultats.

**Structure des r√©sultats pour chaque frame :**

```python
results = model.track(frame, persist=True)

for result in results:
    # --- Bo√Ætes englobantes ---
    boxes = result.boxes
    # boxes.xyxy   ‚Üí Tensor (N, 4) : coordonn√©es [x1, y1, x2, y2]
    # boxes.conf   ‚Üí Tensor (N,)   : confiance de d√©tection
    # boxes.cls    ‚Üí Tensor (N,)   : classe (0 = personne)
    # boxes.id     ‚Üí Tensor (N,)   : ID de suivi ByteTrack (ou None si pas de track)
    
    # --- Keypoints (Squelette) ---
    keypoints = result.keypoints
    # keypoints.data ‚Üí Tensor (N, 17, 3) : [x, y, confidence] pour chaque point
    # keypoints.xy   ‚Üí Tensor (N, 17, 2) : [x, y] uniquement
    # keypoints.conf ‚Üí Tensor (N, 17)    : confiance de chaque point
```

**Sch√©ma du squelette COCO 17 keypoints :**
```
            0 (nez)
           / \
     1 (≈ìil_g)  2 (≈ìil_d)
     |           |
   3 (oreille_g) 4 (oreille_d)
         |
    5 ---+--- 6    (√©paules)
    |         |
    7         8    (coudes)
    |         |
    9        10    (poignets)
    |         |
   11 ---+--- 12   (hanches)
    |         |
   13        14    (genoux)
    |         |
   15        16    (chevilles)
```

---

### 1.3 ‚Äî Cr√©er le Module D√©tecteur (`src/pipeline/detector.py`)

**Actions :**

Cr√©er le fichier `src/pipeline/detector.py` :

```python
"""
Module de d√©tection, suivi et extraction squelettique.
Utilise YOLOv8-Pose avec ByteTrack pour un pipeline unifi√©.
"""
import torch
import numpy as np
from ultralytics import YOLO
from dataclasses import dataclass, field
from typing import List, Optional, Dict
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))
from src.config import (
    YOLO_MODEL, YOLO_CONFIDENCE, YOLO_DEVICE, TRACKER_TYPE
)
from pathlib import Path


@dataclass
class PersonDetection:
    """Repr√©sente une personne d√©tect√©e dans une frame."""
    track_id: int                          # ID de suivi ByteTrack
    bbox: np.ndarray                       # [x1, y1, x2, y2]
    confidence: float                       # Confiance de d√©tection
    keypoints: np.ndarray                  # (17, 3) ‚Üí [x, y, conf]
    keypoints_xy: np.ndarray               # (17, 2) ‚Üí [x, y] uniquement
    name: str = "INCONNU"                  # Sera rempli par InsightFace (√âtape 2)
    action: str = "N/A"                    # Sera rempli par ST-GCN (√âtape 4)

    @property
    def head_bbox(self) -> np.ndarray:
        """Retourne la bo√Æte englobante du tiers sup√©rieur (pour InsightFace)."""
        x1, y1, x2, y2 = self.bbox
        head_height = (y2 - y1) / 3
        return np.array([x1, y1, x2, y1 + head_height])

    @property
    def center(self) -> tuple:
        """Retourne le centre de la bo√Æte englobante."""
        x1, y1, x2, y2 = self.bbox
        return ((x1 + x2) / 2, (y1 + y2) / 2)


class PoseDetector:
    """
    D√©tecteur unifi√© YOLOv8-Pose + ByteTrack.
    Sortie : liste de PersonDetection par frame.
    """

    def __init__(self, model_path: str = YOLO_MODEL, device: int = YOLO_DEVICE):
        """
        Initialise le d√©tecteur.
        
        Args:
            model_path: Chemin vers le mod√®le YOLOv8-Pose
            device: Index du GPU (0) ou "cpu"
        """
        print(f"[DETECTOR] Chargement du mod√®le {model_path}...")
        self.model = YOLO(model_path)
        self.device = device
        self.frame_count = 0
        print(f"[DETECTOR] Mod√®le charg√© avec succ√®s sur device={device}")

    def detect(self, frame: np.ndarray) -> List[PersonDetection]:
        """
        D√©tecte les personnes, suit leur ID, et extrait les squelettes.
        
        Args:
            frame: Image BGR (numpy array) depuis OpenCV
            
        Returns:
            Liste de PersonDetection pour chaque personne d√©tect√©e
        """
        self.frame_count += 1
        detections: List[PersonDetection] = []

        # Inf√©rence YOLOv8-Pose avec suivi ByteTrack
        results = self.model.track(
            source=frame,
            persist=True,          # Maintenir le suivi entre les frames
            tracker=f"{TRACKER_TYPE}.yaml",
            conf=YOLO_CONFIDENCE,
            device=self.device,
            verbose=False          # Pas de log √† chaque frame
        )

        if results is None or len(results) == 0:
            return detections

        result = results[0]

        # V√©rifier que des personnes ont √©t√© d√©tect√©es
        if result.boxes is None or len(result.boxes) == 0:
            return detections

        # V√©rifier que le suivi est actif (IDs disponibles)
        if result.boxes.id is None:
            return detections

        # Extraire les donn√©es
        boxes = result.boxes.xyxy.cpu().numpy()       # (N, 4)
        confs = result.boxes.conf.cpu().numpy()        # (N,)
        track_ids = result.boxes.id.cpu().numpy().astype(int)  # (N,)
        
        # Keypoints
        if result.keypoints is not None:
            kpts_data = result.keypoints.data.cpu().numpy()   # (N, 17, 3)
            kpts_xy = result.keypoints.xy.cpu().numpy()       # (N, 17, 2)
        else:
            return detections

        # Cr√©er les objets PersonDetection
        for i in range(len(boxes)):
            detection = PersonDetection(
                track_id=int(track_ids[i]),
                bbox=boxes[i],
                confidence=float(confs[i]),
                keypoints=kpts_data[i],
                keypoints_xy=kpts_xy[i]
            )
            detections.append(detection)

        return detections

    def get_stats(self) -> dict:
        """Retourne les statistiques du d√©tecteur."""
        return {
            "frames_processed": self.frame_count,
            "model": str(self.model.model_name),
            "device": self.device
        }
```

**‚úÖ Crit√®re de validation 1.3** :
```python
python -c "
from src.pipeline.detector import PoseDetector, PersonDetection
detector = PoseDetector()
print('‚úÖ PoseDetector import√© et initialis√© avec succ√®s')
stats = detector.get_stats()
print(f'  Mod√®le : {stats[\"model\"]}')
print(f'  Device : {stats[\"device\"]}')
"
```

---

### 1.4 ‚Äî Cr√©er le Module d'Affichage (`src/utils/drawing.py`)

**Actions :**

Cr√©er `src/utils/drawing.py` :

```python
"""
Module d'affichage : dessine les bo√Ætes, squelettes et informations sur la vid√©o.
"""
import cv2
import numpy as np
from typing import List

# Connexions du squelette COCO pour dessiner les os
SKELETON_CONNECTIONS = [
    (0, 1), (0, 2),    # Nez ‚Üí Yeux
    (1, 3), (2, 4),    # Yeux ‚Üí Oreilles
    (5, 6),             # √âpaule gauche ‚Üí √âpaule droite
    (5, 7), (7, 9),    # √âpaule G ‚Üí Coude G ‚Üí Poignet G
    (6, 8), (8, 10),   # √âpaule D ‚Üí Coude D ‚Üí Poignet D
    (5, 11), (6, 12),  # √âpaules ‚Üí Hanches
    (11, 12),           # Hanche G ‚Üí Hanche D
    (11, 13), (13, 15), # Hanche G ‚Üí Genou G ‚Üí Cheville G
    (12, 14), (14, 16), # Hanche D ‚Üí Genou D ‚Üí Cheville D
]

# Couleurs pour diff√©rents IDs (palette de 20 couleurs)
COLORS = [
    (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
    (255, 0, 255), (0, 255, 255), (128, 0, 255), (255, 128, 0),
    (0, 128, 255), (128, 255, 0), (255, 0, 128), (0, 255, 128),
    (128, 128, 255), (255, 128, 128), (128, 255, 128), (255, 128, 255),
    (128, 255, 255), (255, 255, 128), (192, 0, 0), (0, 192, 0),
]


def get_color(track_id: int) -> tuple:
    """Retourne une couleur unique bas√©e sur l'ID de suivi."""
    return COLORS[track_id % len(COLORS)]


def draw_detections(frame: np.ndarray, detections: list, 
                     draw_skeleton: bool = True,
                     draw_bbox: bool = True,
                     draw_label: bool = True) -> np.ndarray:
    """
    Dessine les d√©tections sur la frame.
    
    Args:
        frame: Image BGR
        detections: Liste de PersonDetection
        draw_skeleton: Dessiner le squelette
        draw_bbox: Dessiner la bo√Æte englobante
        draw_label: Dessiner le label (ID + nom + action)
    
    Returns:
        Frame annot√©e
    """
    annotated = frame.copy()

    for det in detections:
        color = get_color(det.track_id)
        
        # --- Bo√Æte englobante ---
        if draw_bbox:
            x1, y1, x2, y2 = det.bbox.astype(int)
            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)

        # --- Label ---
        if draw_label:
            x1, y1 = det.bbox[:2].astype(int)
            label = f"ID:{det.track_id} | {det.name}"
            if det.action != "N/A":
                label += f" | {det.action}"
            
            # Fond du texte
            (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
            cv2.rectangle(annotated, (x1, y1 - th - 10), (x1 + tw, y1), color, -1)
            cv2.putText(annotated, label, (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        # --- Squelette ---
        if draw_skeleton:
            kpts = det.keypoints  # (17, 3)
            
            # Dessiner les points
            for j in range(17):
                x, y, conf = kpts[j]
                if conf > 0.5:  # Seuil de confiance du keypoint
                    cv2.circle(annotated, (int(x), int(y)), 4, color, -1)
            
            # Dessiner les connexions
            for (a, b) in SKELETON_CONNECTIONS:
                xa, ya, ca = kpts[a]
                xb, yb, cb = kpts[b]
                if ca > 0.5 and cb > 0.5:
                    cv2.line(annotated, (int(xa), int(ya)), (int(xb), int(yb)), 
                             color, 2)

    return annotated


def draw_fps(frame: np.ndarray, fps: float) -> np.ndarray:
    """Affiche le FPS en haut √† gauche."""
    cv2.putText(frame, f"FPS: {fps:.1f}", (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    return frame


def draw_alert(frame: np.ndarray, message: str, 
               position: tuple = None) -> np.ndarray:
    """Affiche une alerte rouge clignotante."""
    h, w = frame.shape[:2]
    if position is None:
        position = (w // 2 - 200, 50)
    
    # Fond rouge semi-transparent
    overlay = frame.copy()
    cv2.rectangle(overlay, (position[0] - 10, position[1] - 30),
                  (position[0] + 400, position[1] + 10), (0, 0, 200), -1)
    cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
    
    cv2.putText(frame, f"‚ö† {message}", position,
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    return frame
```

**‚úÖ Crit√®re de validation 1.4** :
```python
python -c "
from src.utils.drawing import draw_detections, draw_fps, SKELETON_CONNECTIONS
print(f'‚úÖ Module drawing import√©')
print(f'  Connexions squelette : {len(SKELETON_CONNECTIONS)} os')
"
```

---

### 1.5 ‚Äî Cr√©er le Script de Test Complet

**Actions :**

Cr√©er `tests/test_detector.py` :

```python
"""
Test complet du d√©tecteur YOLOv8-Pose + ByteTrack.
Utilise la webcam ou un fichier vid√©o pour v√©rifier :
- D√©tection des personnes
- Suivi des IDs
- Extraction des squelettes
- Affichage temps r√©el
"""
import cv2
import time
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent))
from src.pipeline.detector import PoseDetector
from src.utils.drawing import draw_detections, draw_fps


def test_with_video(source=0):
    """
    Test le d√©tecteur en temps r√©el.
    
    Args:
        source: 0 pour webcam, ou chemin vers un fichier vid√©o
    """
    print(f"[TEST] Initialisation du d√©tecteur...")
    detector = PoseDetector()
    
    print(f"[TEST] Ouverture de la source vid√©o : {source}")
    cap = cv2.VideoCapture(source)
    
    if not cap.isOpened():
        print(f"‚ùå ERREUR : Impossible d'ouvrir la source vid√©o : {source}")
        return False
    
    fps_counter = 0
    fps_start = time.time()
    current_fps = 0.0
    
    print("[TEST] D√©marrage de la boucle de d√©tection (Appuyez sur 'q' pour quitter)")
    print("=" * 60)
    
    while True:
        ret, frame = cap.read()
        if not ret:
            if isinstance(source, str):
                # Replay la vid√©o en boucle
                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                continue
            break
        
        # --- D√©tection ---
        detections = detector.detect(frame)
        
        # --- Affichage ---
        annotated = draw_detections(frame, detections)
        annotated = draw_fps(annotated, current_fps)
        
        # Infos suppl√©mentaires
        cv2.putText(annotated, f"Personnes: {len(detections)}", (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # Afficher les d√©tails en console (toutes les 30 frames)
        if detector.frame_count % 30 == 0 and detections:
            print(f"\n[Frame {detector.frame_count}] {len(detections)} personne(s) d√©tect√©e(s):")
            for det in detections:
                kpts_valid = sum(1 for k in det.keypoints if k[2] > 0.5)
                print(f"  ID:{det.track_id} | Conf:{det.confidence:.2f} | "
                      f"Keypoints valides: {kpts_valid}/17 | "
                      f"Centre: ({det.center[0]:.0f}, {det.center[1]:.0f})")
        
        # Calcul FPS
        fps_counter += 1
        elapsed = time.time() - fps_start
        if elapsed >= 1.0:
            current_fps = fps_counter / elapsed
            fps_counter = 0
            fps_start = time.time()
        
        cv2.imshow("CCTV AI - Test Detecteur", annotated)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
    
    stats = detector.get_stats()
    print(f"\n{'=' * 60}")
    print(f"[TEST] Termin√©. Statistiques :")
    print(f"  Frames trait√©es : {stats['frames_processed']}")
    print(f"  Mod√®le          : {stats['model']}")
    print(f"  Device           : {stats['device']}")
    
    return True


if __name__ == "__main__":
    # Par d√©faut : webcam (0)
    # Pour un fichier : python test_detector.py chemin/vers/video.mp4
    source = sys.argv[1] if len(sys.argv) > 1 else 0
    
    success = test_with_video(source)
    if success:
        print("\n‚úÖ TEST R√âUSSI ‚Äî Le d√©tecteur fonctionne correctement")
    else:
        print("\n‚ùå TEST √âCHOU√â ‚Äî V√©rifier la configuration")
```

**‚úÖ Crit√®re de validation 1.5** :
```powershell
# Avec une vid√©o :
python tests/test_detector.py data/videos/test.mp4

# OU avec la webcam :
python tests/test_detector.py

# DOIT afficher :
# - Fen√™tre vid√©o avec bo√Ætes, IDs et squelettes
# - FPS affich√© en haut √† gauche
# - Nombre de personnes d√©tect√©es
# - IDs stables (le m√™me ID suit la m√™me personne)
# - Squelettes correctement dessin√©s sur les personnes
```

---

### 1.6 ‚Äî Pr√©parer une Vid√©o de Test

> [!TIP]
> Pour tester sans webcam, t√©l√©chargez une vid√©o de test avec des pi√©tons.

**Actions :**

Option A ‚Äî Utiliser une vid√©o existante :
```powershell
# Placer une vid√©o de surveillance dans le dossier :
# data/videos/test.mp4
```

Option B ‚Äî T√©l√©charger une vid√©o de test gratuite :
```powershell
# Vid√©o pi√©tons depuis Pexels (libre de droits) :
# https://www.pexels.com/search/videos/pedestrian/
# T√©l√©charger et placer dans data/videos/test.mp4
```

Option C ‚Äî Utiliser la webcam :
```powershell
# Utiliser source=0 (webcam par d√©faut)
python tests/test_detector.py 0
```

**‚úÖ Crit√®re de validation 1.6** :
```powershell
# V√©rifier qu'une source vid√©o est disponible :
dir data\videos\
# OU v√©rifier la webcam :
python -c "import cv2; cap = cv2.VideoCapture(0); print('Webcam OK' if cap.isOpened() else 'PAS DE WEBCAM'); cap.release()"
```

---

## ‚úÖ Checklist de Validation Finale ‚Äî √âtape 1

| # | Crit√®re | Commande/Action | Status |
|---|---------|-----------------|--------|
| 1.1 | Mod√®le YOLOv8-Pose t√©l√©charg√© | `yolov8m-pose.pt` charg√© sur GPU | ‚úÖ |
| 1.2 | Structure des r√©sultats comprise | N/A (lecture) | ‚úÖ |
| 1.3 | `detector.py` importable | Import + test frame noire = 0 d√©tections | ‚úÖ |
| 1.4 | `drawing.py` importable | 16 connexions squelette charg√©es | ‚úÖ |
| 1.5 | Test temps r√©el fonctionnel | Script `test_detector.py` cr√©√© | ‚úÖ |
| 1.6 | Source vid√©o disponible | Utiliser webcam ou placer vid√©o dans `data/videos/` | ‚è≥ |

**V√©rifications visuelles obligatoires :**
- [ ] Les bo√Ætes englobantes entourent correctement les personnes
- [ ] Les IDs sont stables (ne changent pas quand la personne ne bouge pas beaucoup)
- [ ] Le squelette est correctement align√© sur le corps de chaque personne
- [ ] Les 17 points cl√©s sont visibles (au moins 12+ pour une personne debout)
- [ ] Le FPS est ‚â• 15 sur la RTX 3080 Ti (‚â• 25 souhait√©)

> [!CAUTION]
> **NE PASSEZ PAS √Ä L'√âTAPE 2** si les squelettes ne sont pas correctement d√©tect√©s. Le ST-GCN (√âtape 4) d√©pend enti√®rement de la qualit√© des keypoints extraits ici.

---

**‚¨ÖÔ∏è √âtape pr√©c√©dente : [etape_0.md](etape_0.md)**
**‚û°Ô∏è √âtape suivante : [etape_2.md](etape_2.md) ‚Äî Identification Visuelle (InsightFace)**
