# ‚ö° √âtape 5 ‚Äî Optimisation pour la RTX 3080 Ti (Pipeline Asynchrone)

## üìã Summary (√Ä lire AVANT de commencer)

**Objectif** : Assembler les 4 briques pr√©c√©dentes (YOLO-Pose + InsightFace + ST-GCN + SQLite) dans un **pipeline asynchrone multithread√©** capable de fonctionner en **temps r√©el √† 30 FPS** sur la RTX 3080 Ti.

**Dur√©e estim√©e** : 3-4 heures

**Pr√©requis** :
- ‚úÖ √âtapes 0 √† 4 enti√®rement valid√©es
- ‚úÖ Chaque module fonctionne isol√©ment
- ‚úÖ GPU CUDA fonctionnel

**Ce que vous aurez √† la fin** :
- ‚úÖ Pipeline 3 threads (Capture ‚Üí D√©tection ‚Üí Analyse)
- ‚úÖ Reconnaissance faciale "paresseuse" (1x / 2s)
- ‚úÖ ST-GCN √† fr√©quence r√©duite (1x / 5 frames)
- ‚úÖ Fichier `main.py` ‚Äî point d'entr√©e unique du syst√®me
- ‚úÖ Performance ‚â• 25 FPS en temps r√©el
- ‚úÖ Syst√®me stable, sans fuites m√©moire

---

## üìù √âtapes D√©taill√©es

### 5.1 ‚Äî Comprendre l'Architecture Multithread√©e

**Pourquoi le multithreading ?**

Sans multithreading, le pipeline est s√©quentiel :
```
Capture (5ms) ‚Üí YOLO (25ms) ‚Üí InsightFace (30ms) ‚Üí ST-GCN (15ms) ‚Üí Affichage (2ms)
= 77ms par frame = ~13 FPS ‚ùå
```

Avec multithreading, les op√©rations se chevauchent :
```
Thread 1 (Capture)     : [‚ñ†][‚ñ†][‚ñ†][‚ñ†][‚ñ†][‚ñ†][‚ñ†][‚ñ†] ‚Üí toujours pr√™t
Thread 2 (YOLO)        :    [‚ñ†‚ñ†‚ñ†][‚ñ†‚ñ†‚ñ†][‚ñ†‚ñ†‚ñ†]       ‚Üí chaque frame
Thread 3 (Analyse)     :       [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†]   [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†]  ‚Üí 1x/5 frames
                                                     
= Frame disponible toutes les ~30ms = ~30 FPS ‚úÖ
```

**Architecture du pipeline :**

```
‚îå‚îÄ‚îÄ Thread 1 ‚îÄ‚îÄ‚îê    Queue 1    ‚îå‚îÄ‚îÄ Thread 2 ‚îÄ‚îÄ‚îê    Queue 2    ‚îå‚îÄ‚îÄ Thread 3 ‚îÄ‚îÄ‚îê
‚îÇ              ‚îÇ               ‚îÇ              ‚îÇ               ‚îÇ              ‚îÇ
‚îÇ  OpenCV      ‚îÇ‚îÄ‚îÄ[frames]‚îÄ‚îÄ‚ñ∏  ‚îÇ  YOLOv8-Pose ‚îÇ‚îÄ‚îÄ[detects]‚îÄ‚îÄ‚ñ∏ ‚îÇ  ST-GCN      ‚îÇ
‚îÇ  Capture     ‚îÇ               ‚îÇ  + ByteTrack ‚îÇ               ‚îÇ  InsightFace ‚îÇ
‚îÇ              ‚îÇ               ‚îÇ              ‚îÇ               ‚îÇ  (lazy)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                      ‚îÇ
                                                                      ‚ñº
                                                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                              ‚îÇ   SQLite     ‚îÇ
                                                              ‚îÇ   Alertes    ‚îÇ
                                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### 5.2 ‚Äî Cr√©er le Module de Capture (`src/pipeline/capture.py`)

**Actions :**

Cr√©er `src/pipeline/capture.py` :

```python
"""
Thread de capture vid√©o.
Lit la vid√©o en continu et place les frames dans une file d'attente.
"""
import cv2
import time
import threading
from queue import Queue
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))
from src.config import VIDEO_WIDTH, VIDEO_HEIGHT, TARGET_FPS


class VideoCapture:
    """
    Capture vid√©o asynchrone.
    
    Lit en continu depuis une source (webcam, fichier, RTSP)
    et garde toujours la frame la plus r√©cente disponible.
    """

    def __init__(self, source=0, queue_size: int = 2):
        """
        Args:
            source: 0 (webcam), chemin fichier, ou URL RTSP
            queue_size: Taille de la file (petit = faible latence)
        """
        self.source = source
        self.cap = cv2.VideoCapture(source)
        
        if not self.cap.isOpened():
            raise RuntimeError(f"Impossible d'ouvrir la source vid√©o : {source}")
        
        # Configurer la r√©solution si c'est une webcam
        if isinstance(source, int):
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, VIDEO_WIDTH)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, VIDEO_HEIGHT)
        
        self.queue = Queue(maxsize=queue_size)
        self.running = False
        self.thread = None
        self.frame_count = 0
        self.fps = 0.0
        
        # Info vid√©o
        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.source_fps = self.cap.get(cv2.CAP_PROP_FPS) or TARGET_FPS
        
        print(f"[CAPTURE] Source : {source}")
        print(f"[CAPTURE] R√©solution : {self.width}x{self.height}")
        print(f"[CAPTURE] FPS source : {self.source_fps}")

    def start(self):
        """D√©marre le thread de capture."""
        self.running = True
        self.thread = threading.Thread(target=self._capture_loop, daemon=True)
        self.thread.start()
        print("[CAPTURE] Thread d√©marr√©")
        return self

    def _capture_loop(self):
        """Boucle de capture (ex√©cut√©e dans un thread s√©par√©)."""
        fps_start = time.time()
        fps_counter = 0
        
        while self.running:
            ret, frame = self.cap.read()
            
            if not ret:
                if isinstance(self.source, str):
                    # Replay vid√©o
                    self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    continue
                else:
                    self.running = False
                    break
            
            # Si la queue est pleine, supprimer la vieille frame
            if self.queue.full():
                try:
                    self.queue.get_nowait()
                except:
                    pass
            
            self.queue.put(frame)
            self.frame_count += 1
            
            # Calcul FPS
            fps_counter += 1
            elapsed = time.time() - fps_start
            if elapsed >= 1.0:
                self.fps = fps_counter / elapsed
                fps_counter = 0
                fps_start = time.time()

    def read(self):
        """
        Lit la frame la plus r√©cente.
        
        Returns:
            (success: bool, frame: np.ndarray)
        """
        if not self.queue.empty():
            return True, self.queue.get()
        return False, None

    def stop(self):
        """Arr√™te le thread de capture."""
        self.running = False
        if self.thread:
            self.thread.join(timeout=5)
        self.cap.release()
        print("[CAPTURE] Thread arr√™t√©")

    def __del__(self):
        self.stop()
```

**‚úÖ Crit√®re de validation 5.2** :
```python
python -c "
import time
from src.pipeline.capture import VideoCapture

cap = VideoCapture(0)  # Webcam
cap.start()

time.sleep(2)  # Laisser tourner 2 secondes
success, frame = cap.read()
assert success, 'Pas de frame re√ßue'
print(f'‚úÖ Capture OK ‚Äî frame shape: {frame.shape}')
print(f'  FPS capture : {cap.fps:.1f}')
print(f'  Frames captur√©es : {cap.frame_count}')

cap.stop()
print('‚úÖ Thread arr√™t√© proprement')
"
```

---

### 5.3 ‚Äî Cr√©er le Module Analyseur (`src/pipeline/analyzer.py`)

> [!IMPORTANT]
> L'analyseur combine le ST-GCN et InsightFace dans un seul thread avec une cadence r√©duite pour ne pas saturer le GPU.

**Actions :**

Cr√©er `src/pipeline/analyzer.py` :

```python
"""
Thread d'analyse : ST-GCN + InsightFace.
Tourne √† fr√©quence r√©duite pour √©conomiser le GPU.

- ST-GCN : 1 inf√©rence toutes les 5 frames
- InsightFace : 1 scan toutes les 2 secondes pour les INCONNUS
"""
import threading
import time
import numpy as np
from queue import Queue
from typing import Dict, List, Optional
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))
from src.config import STGCN_INFERENCE_INTERVAL, FACE_RECOGNITION_INTERVAL
from src.behavior.action_classifier import ActionClassifier
from src.behavior.loitering_detector import LoiteringDetector
from src.face_recognition.matcher import FaceMatcher
from src.database.db_manager import DatabaseManager


class Analyzer:
    """
    Thread d'analyse combinant ST-GCN, InsightFace et la base de donn√©es.
    
    Fonctionne en mode asynchrone : re√ßoit les d√©tections via une queue,
    et met √† jour les r√©sultats dans des dictionnaires thread-safe.
    """

    def __init__(self, frame_width: int = 1920, frame_height: int = 1080):
        """
        Initialise tous les sous-modules d'analyse.
        """
        print("[ANALYZER] Initialisation des modules...")
        
        # ST-GCN
        self.classifier = ActionClassifier(device="cuda")
        
        # Maraudage
        self.loitering = LoiteringDetector()
        self.loitering.set_default_zones(frame_width, frame_height)
        
        # InsightFace
        self.face_matcher = FaceMatcher()
        self.face_matcher.load_whitelist()
        
        # Base de donn√©es
        self.db = DatabaseManager()
        
        # Thread
        self.running = False
        self.thread = None
        self._lock = threading.Lock()
        
        # R√©sultats partag√©s (thread-safe via _lock)
        self.results: Dict[int, dict] = {}  # {track_id: {name, action, alerts}}
        
        print("[ANALYZER] Tous les modules initialis√©s")

    def start(self):
        """D√©marre le thread d'analyse."""
        self.running = True
        self.thread = threading.Thread(target=self._analysis_loop, daemon=True)
        # Note: la boucle est d√©clench√©e depuis le thread principal
        print("[ANALYZER] Pr√™t")
        return self

    def process(self, detections: list, frame: np.ndarray, frame_count: int):
        """
        Traite les d√©tections (appel√© depuis le thread principal ou un worker).
        
        Args:
            detections: Liste de PersonDetection
            frame: Frame courante (pour le crop du visage)
            frame_count: Num√©ro de frame
        """
        active_ids = {d.track_id for d in detections}
        
        for det in detections:
            # --- Mise √† jour ST-GCN buffer ---
            self.classifier.update(det.track_id, det.keypoints_xy)
            
            # --- Mise √† jour BDD ---
            name = self.face_matcher.get_name(det.track_id)
            self.db.update_presence(det.track_id, name)
            
            # --- Reconnaissance faciale (paresseuse) ---
            if self.face_matcher.should_scan(det.track_id, frame_count):
                head = det.head_bbox.astype(int)
                x1 = max(0, head[0])
                y1 = max(0, head[1])
                x2 = min(frame.shape[1], head[2])
                y2 = min(frame.shape[0], head[3])
                
                if x2 > x1 + 20 and y2 > y1 + 20:  # Min 20px
                    face_crop = frame[y1:y2, x1:x2]
                    fname, fscore = self.face_matcher.identify(
                        face_crop, det.track_id, frame_count
                    )
                    if fname != "INCONNU":
                        self.db.update_name(det.track_id, fname)
        
        # --- Inf√©rence ST-GCN (cadence r√©duite) ---
        predictions = self.classifier.classify(frame_count)
        
        # --- V√©rifier maraudage ---
        loitering_alerts = {}
        for det in detections:
            result = self.loitering.update(det.track_id, det.center)
            if result:
                loitering_alerts[det.track_id] = result
        
        # --- V√©rifier alertes ST-GCN ---
        stgcn_alerts = self.classifier.check_alerts()
        
        # --- Compiler les r√©sultats ---
        with self._lock:
            for det in detections:
                tid = det.track_id
                self.results[tid] = {
                    "name": self.face_matcher.get_name(tid),
                    "action": self.classifier.get_action(tid),
                    "prediction": self.classifier.get_prediction(tid),
                    "loitering": loitering_alerts.get(tid, None),
                }
        
        # --- Logger les alertes en BDD ---
        for tid, action, conf in stgcn_alerts:
            name = self.face_matcher.get_name(tid)
            self.db.log_alert(tid, action, conf, name, frame_count)
        
        for tid, (alert_type, duration) in loitering_alerts.items():
            name = self.face_matcher.get_name(tid)
            self.db.log_alert(tid, alert_type, duration, name, frame_count)
        
        # --- Nettoyage ---
        self.classifier.cleanup_lost_ids(active_ids)
        self.face_matcher.cleanup_lost_ids(active_ids)
        self.loitering.cleanup_lost_ids(active_ids)
        
        # V√©rifier les sorties (toutes les 30 frames)
        if frame_count % 30 == 0:
            self.db.check_exits()

    def get_results(self) -> Dict[int, dict]:
        """Retourne les r√©sultats d'analyse (thread-safe)."""
        with self._lock:
            return self.results.copy()

    def apply_to_detections(self, detections: list):
        """Applique les r√©sultats d'analyse aux d√©tections."""
        results = self.get_results()
        for det in detections:
            if det.track_id in results:
                r = results[det.track_id]
                det.name = r["name"]
                det.action = r["action"]
                if r["loitering"]:
                    det.action = f"MARAUDAGE ({r['loitering'][1]:.0f}s)"

    def stop(self):
        """Arr√™te l'analyseur."""
        self.running = False
        self.db.close()
        print("[ANALYZER] Arr√™t√©")

    def get_stats(self) -> dict:
        """Statistiques compl√®tes."""
        return {
            "classifier": self.classifier.get_stats(),
            "face_matcher": self.face_matcher.get_stats(),
            "loitering": self.loitering.get_stats(),
            "database": self.db.get_stats(),
        }
```

**‚úÖ Crit√®re de validation 5.3** :
```python
python -c "
from src.pipeline.analyzer import Analyzer
analyzer = Analyzer()
print('‚úÖ Analyzer initialis√© avec tous les sous-modules')
print(f'  Stats : {analyzer.get_stats()}')
analyzer.stop()
"
```

---

### 5.4 ‚Äî Cr√©er le Point d'Entr√©e Principal (`src/main.py`)

> [!IMPORTANT]
> C'est le fichier final qui assemble tout. Il g√®re la boucle principale, les threads, et l'affichage.

**Actions :**

Cr√©er `src/main.py` :

```python
"""
CCTV AI DEEP SECU ‚Äî Point d'Entr√©e Principal

Pipeline complet :
  Thread 1 : Capture vid√©o (OpenCV)
  Thread 2 : D√©tection + Suivi (YOLOv8-Pose + ByteTrack)
  Thread 3 : Analyse (ST-GCN + InsightFace + SQLite) ‚Äî cadence r√©duite

Usage :
  python src/main.py                          # Webcam
  python src/main.py --source video.mp4       # Fichier vid√©o
  python src/main.py --source rtsp://...      # Flux RTSP
"""
import cv2
import time
import argparse
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent))
from src.config import TARGET_FPS
from src.pipeline.capture import VideoCapture
from src.pipeline.detector import PoseDetector
from src.pipeline.analyzer import Analyzer
from src.utils.drawing import draw_detections, draw_fps, draw_alert


def parse_args():
    """Parse les arguments de la ligne de commande."""
    parser = argparse.ArgumentParser(
        description="CCTV AI DEEP SECU ‚Äî Syst√®me de Vid√©osurveillance Intelligente"
    )
    parser.add_argument(
        "--source", type=str, default="0",
        help="Source vid√©o : 0 (webcam), chemin fichier, ou URL RTSP"
    )
    parser.add_argument(
        "--no-display", action="store_true",
        help="D√©sactiver l'affichage (mode headless)"
    )
    parser.add_argument(
        "--no-face", action="store_true",
        help="D√©sactiver la reconnaissance faciale"
    )
    parser.add_argument(
        "--no-stgcn", action="store_true",
        help="D√©sactiver l'analyse ST-GCN"
    )
    return parser.parse_args()


def main():
    args = parse_args()
    
    # Source vid√©o
    source = int(args.source) if args.source.isdigit() else args.source
    
    print("=" * 60)
    print("  CCTV AI DEEP SECU ‚Äî D√©marrage du syst√®me")
    print("=" * 60)
    
    # === INITIALISATION ===
    print("\n[1/3] Initialisation de la capture vid√©o...")
    capture = VideoCapture(source)
    capture.start()
    
    print("\n[2/3] Initialisation du d√©tecteur YOLOv8-Pose...")
    detector = PoseDetector()
    
    print("\n[3/3] Initialisation de l'analyseur (ST-GCN + InsightFace + DB)...")
    analyzer = Analyzer(
        frame_width=capture.width,
        frame_height=capture.height
    )
    analyzer.start()
    
    print("\n" + "=" * 60)
    print("  ‚úÖ SYST√àME PR√äT ‚Äî Appuyez sur 'q' pour quitter")
    print("  üìä Appuyez sur 's' pour les statistiques")
    print("=" * 60 + "\n")
    
    # === BOUCLE PRINCIPALE ===
    fps_start = time.time()
    fps_counter = 0
    current_fps = 0.0
    
    try:
        while True:
            # 1. Lire la frame
            success, frame = capture.read()
            if not success:
                time.sleep(0.001)
                continue
            
            # 2. D√©tection YOLOv8-Pose (chaque frame)
            detections = detector.detect(frame)
            
            # 3. Analyse (ST-GCN + InsightFace √† cadence r√©duite)
            analyzer.process(detections, frame, detector.frame_count)
            
            # 4. Appliquer les r√©sultats aux d√©tections
            analyzer.apply_to_detections(detections)
            
            # 5. Affichage
            if not args.no_display:
                # Dessiner les zones de maraudage
                frame = analyzer.loitering.draw_zones(frame)
                
                # Dessiner les d√©tections
                annotated = draw_detections(frame, detections)
                annotated = draw_fps(annotated, current_fps)
                
                # Infos suppl√©mentaires
                db_stats = analyzer.db.get_stats()
                cv2.putText(annotated, 
                           f"Personnes: {len(detections)} | "
                           f"DB: {db_stats['currently_present']} present(s) | "
                           f"Alertes: {db_stats['total_alerts']}", 
                           (10, 60),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 200, 0), 2)
                
                # Alertes en cours
                results = analyzer.get_results()
                for tid, r in results.items():
                    if r.get("loitering"):
                        draw_alert(annotated, 
                                  f"MARAUDAGE ID:{tid} ({r['loitering'][1]:.0f}s)")
                
                cv2.imshow("CCTV AI DEEP SECU", annotated)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('s'):
                    print(f"\n{'='*50}")
                    stats = analyzer.get_stats()
                    print(f"üìä STATISTIQUES SYST√àME :")
                    print(f"  FPS          : {current_fps:.1f}")
                    print(f"  Frames       : {detector.frame_count}")
                    print(f"  D√©tections   : {len(detections)}")
                    print(f"  BDD          : {stats['database']}")
                    print(f"  Face Matcher : {stats['face_matcher']}")
                    print(f"  Classifier   : {stats['classifier']}")
                    print(f"  Maraudage    : {stats['loitering']}")
                    print(f"{'='*50}\n")
            
            # Calcul FPS
            fps_counter += 1
            elapsed = time.time() - fps_start
            if elapsed >= 1.0:
                current_fps = fps_counter / elapsed
                fps_counter = 0
                fps_start = time.time()
    
    except KeyboardInterrupt:
        print("\n[SYSTEM] Interruption utilisateur...")
    
    finally:
        # === ARR√äT PROPRE ===
        print("\n[SYSTEM] Arr√™t du syst√®me...")
        capture.stop()
        analyzer.stop()
        cv2.destroyAllWindows()
        
        print(f"\n{'='*50}")
        print(f"  SESSION TERMIN√âE")
        print(f"  Frames trait√©es : {detector.frame_count}")
        print(f"  FPS moyen       : {current_fps:.1f}")
        print(f"{'='*50}")


if __name__ == "__main__":
    main()
```

**‚úÖ Crit√®re de validation 5.4** :
```powershell
# Lancer le syst√®me complet :
python src/main.py --source 0

# OU avec une vid√©o :
python src/main.py --source data/videos/test.mp4

# DOIT :
# - Afficher la fen√™tre avec bo√Ætes + squelettes + noms + actions
# - FPS ‚â• 25 affich√© en haut √† gauche
# - Zones de maraudage visibles
# - Pas de crash ni leak m√©moire
# - 'q' pour quitter proprement
# - 's' pour les stats dans la console
```

---

### 5.5 ‚Äî Optimisations de Performance

**Actions de fine-tuning :**

#### A) Profiler le GPU
```python
# Ajouter dans main.py pour diagnostiquer un bottleneck GPU :
import torch
print(f"GPU Memory: {torch.cuda.memory_allocated()/1024**2:.0f} MB / "
      f"{torch.cuda.get_device_properties(0).total_mem/1024**2:.0f} MB")
```

#### B) R√©duire la r√©solution si n√©cessaire
```python
# Dans config.py, passer en 720p si le FPS est trop bas :
VIDEO_WIDTH = 1280
VIDEO_HEIGHT = 720
```

#### C) Ajuster les intervalles d'inf√©rence
```python
# Dans config.py :
STGCN_INFERENCE_INTERVAL = 10   # Au lieu de 5 (√©conomise 50% GPU ST-GCN)
FACE_RECOGNITION_INTERVAL = 120  # Au lieu de 60 (√©conomise 50% GPU InsightFace)
```

#### D) Utiliser le half-precision (FP16)
```python
# Dans detector.py, ajouter half=True :
results = self.model.track(
    source=frame,
    persist=True,
    half=True,  # FP16 ‚Üí 2x plus rapide sur RTX 3080 Ti
    ...
)
```

#### E) Benchmarker
```python
# Script de benchmark rapide : tests/benchmark.py
import time
import torch
import cv2
from src.pipeline.detector import PoseDetector

detector = PoseDetector()
cap = cv2.VideoCapture(0)

times = []
for _ in range(100):
    ret, frame = cap.read()
    t0 = time.time()
    detections = detector.detect(frame)
    times.append(time.time() - t0)

cap.release()
avg = sum(times) / len(times) * 1000
print(f"Temps moyen YOLO-Pose : {avg:.1f} ms/frame")
print(f"FPS th√©orique max    : {1000/avg:.0f}")
print(f"GPU Memory           : {torch.cuda.memory_allocated()/1024**2:.0f} MB")
```

**‚úÖ Crit√®re de validation 5.5** :
```powershell
python tests/benchmark.py

# R√©sultats attendus sur RTX 3080 Ti :
# Temps moyen YOLO-Pose : 15-25 ms/frame
# FPS th√©orique max    : 40-65
# GPU Memory           : < 4000 MB (laissant de la marge pour ST-GCN + InsightFace)
```

---

### 5.6 ‚Äî Cr√©er le Script de Lancement Windows (`setup_env.bat`)

**Actions :**

Cr√©er `setup_env.bat` √† la racine du projet :

```batch
@echo off
echo ============================================
echo   CCTV AI DEEP SECU ‚Äî Setup Environment
echo ============================================
echo.

REM V√©rifier Python
python --version 2>NUL
if %ERRORLEVEL% NEQ 0 (
    echo ‚ùå Python non trouv√© ! Installer Python 3.10 ou 3.11
    pause
    exit /b 1
)

REM V√©rifier CUDA
nvcc --version 2>NUL
if %ERRORLEVEL% NEQ 0 (
    echo ‚ö† CUDA non trouv√©. Les mod√®les tourneront sur CPU (tr√®s lent)
)

REM Cr√©er l'environnement virtuel
if not exist venv (
    echo [1/4] Cr√©ation de l'environnement virtuel...
    python -m venv venv
) else (
    echo [1/4] Environnement virtuel existant trouv√©
)

REM Activer
call venv\Scripts\activate.bat

REM Installer les d√©pendances
echo [2/4] Installation de PyTorch avec CUDA...
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

echo [3/4] Installation des d√©pendances...
pip install -r requirements.txt

echo [4/4] V√©rification...
python -c "import torch; print(f'PyTorch {torch.__version__} - CUDA: {torch.cuda.is_available()}')"

echo.
echo ============================================
echo   ‚úÖ Setup termin√© !
echo   Pour lancer : python src/main.py
echo ============================================
pause
```

**‚úÖ Crit√®re de validation 5.6** :
```powershell
# Le script doit s'ex√©cuter sans erreur :
.\setup_env.bat
```

---

## ‚úÖ Checklist de Validation Finale ‚Äî √âtape 5

| # | Crit√®re | Commande/Action | Status |
|---|---------|-----------------|--------|
| 5.1 | Architecture multithread√©e comprise | N/A (lecture) | ‚¨ú |
| 5.2 | `capture.py` fonctionnel | Test de capture asynchrone | ‚¨ú |
| 5.3 | `analyzer.py` fonctionnel | Initialisation r√©ussie | ‚¨ú |
| 5.4 | `main.py` fonctionnel | Syst√®me complet √† 25+ FPS | ‚¨ú |
| 5.5 | Performance optimis√©e | Benchmark < 25ms/frame | ‚¨ú |
| 5.6 | `setup_env.bat` fonctionnel | Setup sans erreur | ‚¨ú |

**V√©rifications de performance obligatoires :**
- [ ] FPS ‚â• 25 sur la RTX 3080 Ti (id√©al: ‚â• 30)
- [ ] M√©moire GPU < 8 Go (sur 12 Go disponibles)
- [ ] Pas de fuite m√©moire (surveiller sur 5+ minutes)
- [ ] Le syst√®me g√®re correctement l'entr√©e/sortie de personnes du champ
- [ ] Les alertes apparaissent en overlay et sont logu√©es en BDD
- [ ] Le syst√®me s'arr√™te proprement avec 'q' ou Ctrl+C

**Tests de stress :**
- [ ] 1 personne ‚Üí fonctionne
- [ ] 3-5 personnes ‚Üí fonctionne avec FPS acceptable
- [ ] Personne entre et sort ‚Üí IDs stables, BDD √† jour
- [ ] Syst√®me tourne 10+ minutes sans crash

> [!CAUTION]
> Si le FPS tombe sous 15 avec plusieurs personnes :
> 1. R√©duire la r√©solution (720p au lieu de 1080p)
> 2. Augmenter `STGCN_INFERENCE_INTERVAL` √† 10 ou 15
> 3. Augmenter `FACE_RECOGNITION_INTERVAL` √† 120
> 4. Activer le half-precision (FP16) dans YOLO

---

## üéâ Projet Complet !

Si vous √™tes arriv√© ici avec tous les crit√®res valid√©s, votre syst√®me CCTV AI DEEP SECU est op√©rationnel :

```
‚úÖ Phase 0 : Environnement configur√© (CUDA + PyTorch + GPU)
‚úÖ Phase 1 : D√©tection + Suivi + Squelette (YOLOv8-Pose + ByteTrack)
‚úÖ Phase 2 : Identification faciale (InsightFace + Liste blanche)
‚úÖ Phase 3 : Base de donn√©es (SQLite + Historique + Alertes)
‚úÖ Phase 4 : Analyse comportementale (ST-GCN + Maraudage)
‚úÖ Phase 5 : Pipeline optimis√© (30 FPS sur RTX 3080 Ti)
```

**Commande de lancement finale :**
```powershell
cd C:\Users\ilyas\Documents\CCTV_AI_DEEP_SECU
.\venv\Scripts\Activate.ps1
python src/main.py --source 0
```

---

**‚¨ÖÔ∏è √âtape pr√©c√©dente : [etape_4.md](etape_4.md)**
**üìã Vue d'ensemble : [overview.md](overview.md)**
