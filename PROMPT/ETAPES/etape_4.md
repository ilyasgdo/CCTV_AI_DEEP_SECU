# ğŸ§  Ã‰tape 4 â€” Analyse Comportementale via ST-GCN (Le CÅ“ur du SystÃ¨me)

## ğŸ“‹ Summary (Ã€ lire AVANT de commencer)

**Objectif** : ImplÃ©menter l'analyse comportementale par **ST-GCN** (Spatial Temporal Graph Convolutional Network). Le systÃ¨me crÃ©e un "buffer temporel" pour chaque personne (30 derniÃ¨res frames de squelette) et le soumet au ST-GCN pour classifier l'action en cours (marcher, chute, coup, immobile...). En complÃ©ment, une rÃ¨gle spatiale de **maraudage** dÃ©tecte les personnes restant trop longtemps dans une zone dÃ©finie.

**DurÃ©e estimÃ©e** : 3-5 heures (la plus complexe de toutes les Ã©tapes)

**PrÃ©requis** :
- âœ… Ã‰tapes 0, 1, 2 et 3 entiÃ¨rement validÃ©es
- âœ… Le dÃ©tecteur extrait les 17 keypoints COCO de maniÃ¨re fiable
- âœ… PyTorch fonctionne avec CUDA
- âœ… Connexion internet (pour tÃ©lÃ©charger le modÃ¨le ST-GCN prÃ©-entraÃ®nÃ©)

**Ce que vous aurez Ã  la fin** :
- âœ… Buffer temporel (`deque`) par personne avec 30 frames de squelette
- âœ… ModÃ¨le ST-GCN chargÃ© et fonctionnel sur GPU
- âœ… Classification d'actions en temps rÃ©el (marcher, chute, coup...)
- âœ… DÃ©tection de maraudage par polygone spatial
- âœ… Alertes visuelles et en base de donnÃ©es
- âœ… Test complet validÃ©

---

## ğŸ“ Ã‰tapes DÃ©taillÃ©es

### 4.1 â€” Comprendre le ST-GCN (ThÃ©orie Essentielle)

> [!NOTE]
> **Pourquoi le ST-GCN est supÃ©rieur aux rÃ¨gles codÃ©es en dur ?**
> 
> - **RÃ¨gle codÃ©e** : "Si Ã©paules sous les genoux â†’ chute" â†’ Trop de faux positifs (la personne se penche pour ramasser quelque chose)
> - **ST-GCN** : Analyse la **sÃ©quence temporelle** du squelette sur 1-2 secondes â†’ Comprend la **dynamique** du mouvement (vitesse de descente, angle, trajectoire) â†’ Distingue une chute d'un simple mouvement de flexion

**Comment Ã§a fonctionne :**

```
Frame t-29  Frame t-28  ...  Frame t-1   Frame t
   ğŸ§          ğŸ§               ğŸ§         ğŸ§
   â”‚           â”‚                â”‚          â”‚
   â””â”€â”€â”€ Le ST-GCN analyse l'Ã©volution du squelette â”€â”€â”€â”˜
                    sur 30 frames
                         â”‚
                         â–¼
              [Marche: 5%, Chute: 93%, ...]
```

Le ST-GCN traite le squelette comme un **graphe** :
- **NÅ“uds** = les 17 keypoints
- **ArÃªtes spatiales** = les connexions du squelette (Ã©paule-coude, etc.)
- **ArÃªtes temporelles** = le mÃªme keypoint entre 2 frames successives

---

### 4.2 â€” TÃ©lÃ©charger et PrÃ©parer le ModÃ¨le ST-GCN PrÃ©-entraÃ®nÃ©

> [!IMPORTANT]
> Plusieurs options existent pour le modÃ¨le. On recommande **2s-AGCN** ou le ST-GCN original, prÃ©-entraÃ®nÃ© sur **NTU-RGB+D** (60 classes d'actions humaines incluant chutes, coups, etc.).

**Option A â€” ST-GCN Original (RecommandÃ© pour commencer) :**

```powershell
# Cloner le dÃ©pÃ´t ST-GCN dans un dossier temporaire
cd C:\Users\ilyas\Documents\CCTV_AI_DEEP_SECU
git clone https://github.com/yysijie/st-gcn.git external/st-gcn
```

**Option B â€” Utiliser pyskl (Plus moderne, plus de modÃ¨les) :**

```powershell
pip install pyskl
# OU
git clone https://github.com/kennymckormick/pyskl.git external/pyskl
```

**Option C â€” ImplÃ©menter un ST-GCN simplifiÃ© (ContrÃ´le total) :**

Nous allons implÃ©menter notre propre version simplifiÃ©e pour avoir le contrÃ´le total et Ã©viter les dÃ©pendances lourdes.

**âœ… CritÃ¨re de validation 4.2** :
```powershell
# VÃ©rifier que le dossier externe existe :
dir external\
# OU vÃ©rifier que pyskl est installÃ© :
python -c "import pyskl; print('pyskl OK')"
```

---

### 4.3 â€” CrÃ©er le Module de Buffer Temporel (`src/behavior/skeleton_buffer.py`)

**Actions :**

CrÃ©er `src/behavior/skeleton_buffer.py` :

```python
"""
Buffer temporel pour stocker l'historique des squelettes.
Utilise collections.deque pour une mÃ©moire glissante efficace.
"""
import numpy as np
from collections import deque
from typing import Dict, Optional
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent.parent))
from src.config import STGCN_BUFFER_SIZE, STGCN_NUM_KEYPOINTS, STGCN_IN_CHANNELS


class SkeletonBuffer:
    """
    GÃ¨re le buffer temporel d'un seul individu.
    
    Stocke les N derniÃ¨res frames de keypoints sous forme de deque.
    Quand le buffer est plein, les anciennes frames sont automatiquement
    supprimÃ©es (FIFO).
    """

    def __init__(self, buffer_size: int = STGCN_BUFFER_SIZE,
                 num_keypoints: int = STGCN_NUM_KEYPOINTS,
                 in_channels: int = STGCN_IN_CHANNELS):
        """
        Args:
            buffer_size: Nombre de frames Ã  garder (30 â‰ˆ 1s Ã  30fps)
            num_keypoints: Nombre de keypoints (17 COCO)
            in_channels: Nombre de canaux par keypoint (2 = X,Y ou 3 = X,Y,conf)
        """
        self.buffer_size = buffer_size
        self.num_keypoints = num_keypoints
        self.in_channels = in_channels
        self.buffer: deque = deque(maxlen=buffer_size)

    def add_frame(self, keypoints_xy: np.ndarray):
        """
        Ajoute les keypoints d'une frame au buffer.
        
        Args:
            keypoints_xy: Array (17, 2) ou (17, 3) des coordonnÃ©es
        """
        if keypoints_xy.shape[0] != self.num_keypoints:
            raise ValueError(
                f"Attendu {self.num_keypoints} keypoints, "
                f"reÃ§u {keypoints_xy.shape[0]}"
            )
        # Ne garder que les canaux voulus (X, Y) ou (X, Y, conf)
        kpts = keypoints_xy[:, :self.in_channels].copy()
        self.buffer.append(kpts)

    @property
    def is_ready(self) -> bool:
        """Le buffer est-il plein (prÃªt pour l'infÃ©rence) ?"""
        return len(self.buffer) == self.buffer_size

    @property
    def fill_ratio(self) -> float:
        """Ratio de remplissage (0.0 Ã  1.0)."""
        return len(self.buffer) / self.buffer_size

    def to_numpy(self) -> Optional[np.ndarray]:
        """
        Convertit le buffer en array numpy pour le ST-GCN.
        
        Returns:
            Array (C, T, V) = (in_channels, buffer_size, num_keypoints)
            ou None si le buffer n'est pas prÃªt
        """
        if not self.is_ready:
            return None
        
        # Stack les frames : (T, V, C)
        data = np.stack(list(self.buffer), axis=0)  # (T, 17, 2)
        
        # Normaliser les coordonnÃ©es (centrer sur la hanche)
        # Centre = moyenne des hanches gauche (11) et droite (12)
        hip_center = (data[:, 11, :] + data[:, 12, :]) / 2  # (T, 2)
        data = data - hip_center[:, np.newaxis, :]  # Centrer
        
        # Normaliser par la taille du squelette
        # Utiliser la distance Ã©paule-hanche comme rÃ©fÃ©rence
        scale = np.mean(np.linalg.norm(
            data[:, 5, :] - data[:, 11, :], axis=-1
        ))
        if scale > 0:
            data = data / scale
        
        # Transposer en (C, T, V) pour le ST-GCN
        data = data.transpose(2, 0, 1)  # (2, 30, 17)
        
        return data.astype(np.float32)

    def reset(self):
        """Vide le buffer."""
        self.buffer.clear()


class MultiPersonBuffer:
    """
    GÃ¨re les buffers temporels de TOUTES les personnes Ã  l'Ã©cran.
    CrÃ©e automatiquement un buffer par track_id.
    """

    def __init__(self, buffer_size: int = STGCN_BUFFER_SIZE):
        self.buffer_size = buffer_size
        self.buffers: Dict[int, SkeletonBuffer] = {}

    def update(self, track_id: int, keypoints_xy: np.ndarray):
        """
        Met Ã  jour le buffer d'une personne.
        CrÃ©e le buffer si c'est un nouvel ID.
        """
        if track_id not in self.buffers:
            self.buffers[track_id] = SkeletonBuffer(self.buffer_size)
        self.buffers[track_id].add_frame(keypoints_xy)

    def get_ready_buffers(self) -> Dict[int, np.ndarray]:
        """
        Retourne les buffers prÃªts pour l'infÃ©rence ST-GCN.
        
        Returns:
            Dict {track_id: array (C, T, V)}
        """
        ready = {}
        for track_id, buf in self.buffers.items():
            if buf.is_ready:
                data = buf.to_numpy()
                if data is not None:
                    ready[track_id] = data
        return ready

    def cleanup_lost_ids(self, active_ids: set):
        """Supprime les buffers des personnes disparues."""
        lost = set(self.buffers.keys()) - active_ids
        for tid in lost:
            del self.buffers[tid]

    def get_stats(self) -> dict:
        """Retourne les statistiques des buffers."""
        return {
            "total_buffers": len(self.buffers),
            "ready_buffers": sum(1 for b in self.buffers.values() if b.is_ready),
            "fill_ratios": {
                tid: f"{buf.fill_ratio:.0%}" 
                for tid, buf in self.buffers.items()
            }
        }
```

**âœ… CritÃ¨re de validation 4.3** :
```python
python -c "
import numpy as np
from src.behavior.skeleton_buffer import SkeletonBuffer, MultiPersonBuffer

# Test SkeletonBuffer
buf = SkeletonBuffer(buffer_size=30)
assert not buf.is_ready

# Remplir avec des donnÃ©es simulÃ©es
for i in range(30):
    kpts = np.random.randn(17, 2).astype(np.float32) * 100
    kpts += [960, 540]  # Centrer dans une image 1920x1080
    buf.add_frame(kpts)

assert buf.is_ready
data = buf.to_numpy()
assert data.shape == (2, 30, 17), f'Shape attendu (2, 30, 17), obtenu {data.shape}'
print(f'âœ… SkeletonBuffer OK â€” shape: {data.shape}')

# Test MultiPersonBuffer
multi = MultiPersonBuffer()
for frame in range(35):
    multi.update(1, np.random.randn(17, 2).astype(np.float32))
    multi.update(2, np.random.randn(17, 2).astype(np.float32))

ready = multi.get_ready_buffers()
assert len(ready) == 2
print(f'âœ… MultiPersonBuffer OK â€” {len(ready)} buffers prÃªts')
print(f'  Stats : {multi.get_stats()}')
"
```

---

### 4.4 â€” ImplÃ©menter le ModÃ¨le ST-GCN (`src/models/stgcn/model.py`)

> [!IMPORTANT]
> Nous implÃ©mentons un ST-GCN simplifiÃ© mais fonctionnel. Pour un systÃ¨me de production, utilisez un modÃ¨le prÃ©-entraÃ®nÃ© sur NTU-RGB+D.

**Actions :**

CrÃ©er `src/models/stgcn/model.py` :

```python
"""
ImplÃ©mentation du ST-GCN (Spatial Temporal Graph Convolutional Network).
BasÃ© sur l'architecture originale de Yan et al. (2018).

Architecture :
    Input: (N, C, T, V) = (batch, channels, frames, keypoints)
    â†’ Couches ST-GCN empilÃ©es (convolution spatiale sur le graphe + convolution temporelle)
    â†’ Global Average Pooling
    â†’ Couche Dense â†’ Classification

Graphe COCO (17 keypoints) :
    Les connexions sont dÃ©finies par la matrice d'adjacence.
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).parent.parent.parent.parent))
from src.config import STGCN_NUM_KEYPOINTS, STGCN_IN_CHANNELS, ACTION_LABELS


# === GRAPHE COCO 17 KEYPOINTS ===
# DÃ©finition des connexions du squelette
COCO_CONNECTIONS = [
    (0, 1), (0, 2),       # Nez â†’ Yeux
    (1, 3), (2, 4),       # Yeux â†’ Oreilles
    (0, 5), (0, 6),       # Nez â†’ Ã‰paules (approximation)
    (5, 7), (7, 9),       # Ã‰paule G â†’ Coude G â†’ Poignet G
    (6, 8), (8, 10),      # Ã‰paule D â†’ Coude D â†’ Poignet D
    (5, 11), (6, 12),     # Ã‰paules â†’ Hanches
    (11, 13), (13, 15),   # Hanche G â†’ Genou G â†’ Cheville G
    (12, 14), (14, 16),   # Hanche D â†’ Genou D â†’ Cheville D
    (11, 12),             # Hanche G â†’ Hanche D
    (5, 6),               # Ã‰paule G â†’ Ã‰paule D
]


def build_adjacency_matrix(num_nodes: int = 17, 
                            connections: list = None) -> np.ndarray:
    """
    Construit la matrice d'adjacence normalisÃ©e du graphe squelettique.
    
    Args:
        num_nodes: Nombre de nÅ“uds (keypoints)
        connections: Liste de tuples (i, j) des connexions
        
    Returns:
        Matrice d'adjacence normalisÃ©e (V, V)
    """
    if connections is None:
        connections = COCO_CONNECTIONS
    
    # Matrice d'adjacence avec self-loops
    A = np.eye(num_nodes, dtype=np.float32)
    for (i, j) in connections:
        A[i, j] = 1
        A[j, i] = 1
    
    # Normalisation par le degrÃ© (D^{-1/2} * A * D^{-1/2})
    D = np.sum(A, axis=1)
    D_inv_sqrt = np.diag(1.0 / np.sqrt(D + 1e-8))
    A_norm = D_inv_sqrt @ A @ D_inv_sqrt
    
    return A_norm


class GraphConvolution(nn.Module):
    """Convolution sur le graphe spatial."""
    
    def __init__(self, in_channels: int, out_channels: int, A: torch.Tensor):
        super().__init__()
        self.A = A  # (V, V) matrice d'adjacence
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.bn = nn.BatchNorm2d(out_channels)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (N, C, T, V) â€” batch, channels, time, vertices
        Returns:
            (N, C_out, T, V)
        """
        # Multiplication par la matrice d'adjacence : agrÃ©gation spatiale
        # x @ A^T â†’ message passing sur le graphe
        x = torch.einsum('nctv,vw->nctw', x, self.A.to(x.device))
        x = self.conv(x)
        x = self.bn(x)
        return x


class STGCNBlock(nn.Module):
    """
    Bloc ST-GCN = Convolution Spatiale (Graph) + Convolution Temporelle + RÃ©sidu.
    """
    
    def __init__(self, in_channels: int, out_channels: int, 
                 A: torch.Tensor, stride: int = 1):
        super().__init__()
        
        # Convolution spatiale (sur le graphe)
        self.gcn = GraphConvolution(in_channels, out_channels, A)
        
        # Convolution temporelle (1D le long de l'axe T)
        self.tcn = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, 
                      kernel_size=(9, 1), padding=(4, 0), stride=(stride, 1)),
            nn.BatchNorm2d(out_channels),
        )
        
        # Connexion rÃ©siduelle
        if in_channels != out_channels or stride != 1:
            self.residual = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 
                          kernel_size=1, stride=(stride, 1)),
                nn.BatchNorm2d(out_channels),
            )
        else:
            self.residual = nn.Identity()
        
        self.relu = nn.ReLU(inplace=True)
        self.dropout = nn.Dropout(0.25)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (N, C_in, T, V)
        Returns:
            (N, C_out, T', V) â€” T' = T/stride
        """
        res = self.residual(x)
        x = self.gcn(x)
        x = self.relu(x)
        x = self.tcn(x)
        x = self.dropout(x)
        x = x + res
        x = self.relu(x)
        return x


class STGCN(nn.Module):
    """
    ST-GCN complet pour la classification d'actions.
    
    Architecture :
        Input (N, 2, 30, 17)
        â†’ 3 blocs ST-GCN (64, 128, 256 channels)
        â†’ Global Average Pooling
        â†’ FC â†’ num_classes
    """
    
    def __init__(self, in_channels: int = STGCN_IN_CHANNELS, 
                 num_classes: int = None,
                 num_keypoints: int = STGCN_NUM_KEYPOINTS):
        super().__init__()
        
        if num_classes is None:
            num_classes = len(ACTION_LABELS)
        
        # Matrice d'adjacence du squelette COCO
        A_np = build_adjacency_matrix(num_keypoints)
        self.register_buffer('A', torch.tensor(A_np, dtype=torch.float32))
        
        # Normalisation d'entrÃ©e
        self.data_bn = nn.BatchNorm1d(in_channels * num_keypoints)
        
        # Couches ST-GCN
        self.layers = nn.ModuleList([
            STGCNBlock(in_channels, 64, self.A),
            STGCNBlock(64, 64, self.A),
            STGCNBlock(64, 128, self.A, stride=2),    # T: 30 â†’ 15
            STGCNBlock(128, 128, self.A),
            STGCNBlock(128, 256, self.A, stride=2),    # T: 15 â†’ 8
            STGCNBlock(256, 256, self.A),
        ])
        
        # Classificateur
        self.fc = nn.Linear(256, num_classes)
        
        self.num_classes = num_classes
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (N, C, T, V) = (batch, 2, 30, 17)
            
        Returns:
            (N, num_classes) â€” probabilitÃ©s pour chaque action
        """
        N, C, T, V = x.shape
        
        # Batch normalization sur l'entrÃ©e
        x_bn = x.permute(0, 3, 1, 2).contiguous().view(N, V * C, T)
        x_bn = self.data_bn(x_bn)
        x = x_bn.view(N, V, C, T).permute(0, 2, 3, 1).contiguous()
        
        # Couches ST-GCN
        for layer in self.layers:
            x = layer(x)
        
        # Global Average Pooling
        x = F.adaptive_avg_pool2d(x, (1, 1))  # (N, 256, 1, 1)
        x = x.view(N, -1)                      # (N, 256)
        
        # Classification
        x = self.fc(x)  # (N, num_classes)
        
        return x
    
    def predict(self, x: torch.Tensor) -> dict:
        """
        Fait une prÃ©diction avec les probabilitÃ©s par action.
        
        Args:
            x: (1, C, T, V) â€” un seul Ã©chantillon
            
        Returns:
            Dict {action_name: probability}
        """
        self.eval()
        with torch.no_grad():
            logits = self.forward(x)
            probs = F.softmax(logits, dim=-1)[0]
        
        result = {}
        for i, label in enumerate(ACTION_LABELS):
            result[label] = float(probs[i])
        
        return result
```

**âœ… CritÃ¨re de validation 4.4** :
```python
python -c "
import torch
from src.models.stgcn.model import STGCN, build_adjacency_matrix

# VÃ©rifier la matrice d'adjacence
A = build_adjacency_matrix()
assert A.shape == (17, 17)
print(f'âœ… Matrice d adjacence : {A.shape}')

# CrÃ©er le modÃ¨le
model = STGCN(in_channels=2, num_classes=8)
print(f'âœ… ModÃ¨le ST-GCN crÃ©Ã©')
print(f'  ParamÃ¨tres : {sum(p.numel() for p in model.parameters()):,}')

# Test avec des donnÃ©es simulÃ©es
x = torch.randn(1, 2, 30, 17)  # 1 personne, 2 canaux, 30 frames, 17 keypoints
out = model(x)
assert out.shape == (1, 8), f'Shape attendu (1, 8), obtenu {out.shape}'
print(f'âœ… Forward pass OK â€” sortie : {out.shape}')

# Test predict
preds = model.predict(x)
print(f'âœ… PrÃ©dictions : {preds}')
total_prob = sum(preds.values())
print(f'  Somme des probabilitÃ©s : {total_prob:.4f} (â‰ˆ 1.0)')
"
```

---

### 4.5 â€” CrÃ©er le Classificateur d'Actions (`src/behavior/action_classifier.py`)

**Actions :**

CrÃ©er `src/behavior/action_classifier.py` :

```python
"""
Interface de haut niveau pour le classificateur d'actions ST-GCN.
GÃ¨re le chargement du modÃ¨le, l'infÃ©rence, et les alertes.
"""
import torch
import numpy as np
from typing import Dict, Optional, Tuple
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))
from src.config import (
    STGCN_INFERENCE_INTERVAL, ALERT_ACTIONS, 
    ACTION_LABELS, STGCN_BUFFER_SIZE
)
from src.models.stgcn.model import STGCN
from src.behavior.skeleton_buffer import MultiPersonBuffer


class ActionClassifier:
    """
    Classificateur d'actions utilisant le ST-GCN.
    
    GÃ¨re :
    - Le buffer temporel par personne
    - L'infÃ©rence pÃ©riodique (toutes les N frames)
    - La dÃ©tection d'alertes
    """

    def __init__(self, weights_path: str = None, device: str = "cuda"):
        """
        Args:
            weights_path: Chemin vers les poids prÃ©-entraÃ®nÃ©s (None = modÃ¨le non entraÃ®nÃ©)
            device: "cuda" ou "cpu"
        """
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        
        # Initialiser le modÃ¨le
        self.model = STGCN(
            in_channels=2,
            num_classes=len(ACTION_LABELS)
        ).to(self.device)
        
        # Charger les poids prÃ©-entraÃ®nÃ©s si disponibles
        if weights_path and Path(weights_path).exists():
            print(f"[ACTION] Chargement des poids : {weights_path}")
            state_dict = torch.load(weights_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
            print("[ACTION] Poids chargÃ©s avec succÃ¨s")
        else:
            print("[ACTION] âš  ModÃ¨le non entraÃ®nÃ© (poids alÃ©atoires)")
            print("[ACTION]   Pour un systÃ¨me de production, entraÃ®ner sur NTU-RGB+D")
        
        self.model.eval()
        
        # Buffer multi-personnes
        self.buffer = MultiPersonBuffer(buffer_size=STGCN_BUFFER_SIZE)
        
        # Cache des derniÃ¨res prÃ©dictions
        self._last_predictions: Dict[int, Dict[str, float]] = {}
        self._last_actions: Dict[int, str] = {}
        self._frame_count = 0

    def update(self, track_id: int, keypoints_xy: np.ndarray):
        """
        Met Ã  jour le buffer d'une personne avec les nouvelles keypoints.
        
        Args:
            track_id: ID de suivi
            keypoints_xy: (17, 2) array des coordonnÃ©es
        """
        self.buffer.update(track_id, keypoints_xy)

    def should_infer(self, frame_count: int) -> bool:
        """DÃ©termine si c'est le moment de lancer l'infÃ©rence ST-GCN."""
        return frame_count % STGCN_INFERENCE_INTERVAL == 0

    def classify(self, frame_count: int) -> Dict[int, Dict[str, float]]:
        """
        Lance l'infÃ©rence ST-GCN pour toutes les personnes avec un buffer plein.
        
        Args:
            frame_count: NumÃ©ro de frame actuel
            
        Returns:
            Dict {track_id: {action: probability, ...}}
        """
        self._frame_count = frame_count
        
        if not self.should_infer(frame_count):
            return self._last_predictions

        # RÃ©cupÃ©rer les buffers prÃªts
        ready_buffers = self.buffer.get_ready_buffers()
        
        if not ready_buffers:
            return self._last_predictions

        # PrÃ©parer le batch
        track_ids = list(ready_buffers.keys())
        batch = np.stack([ready_buffers[tid] for tid in track_ids])  # (B, C, T, V)
        
        # Convertir en tenseur PyTorch
        tensor = torch.tensor(batch, dtype=torch.float32).to(self.device)
        
        # InfÃ©rence
        with torch.no_grad():
            logits = self.model(tensor)
            probs = torch.softmax(logits, dim=-1).cpu().numpy()
        
        # Stocker les rÃ©sultats
        for i, tid in enumerate(track_ids):
            prediction = {}
            for j, label in enumerate(ACTION_LABELS):
                prediction[label] = float(probs[i, j])
            
            self._last_predictions[tid] = prediction
            self._last_actions[tid] = max(prediction, key=prediction.get)

        return self._last_predictions

    def get_action(self, track_id: int) -> str:
        """Retourne la derniÃ¨re action prÃ©dite pour un track_id."""
        return self._last_actions.get(track_id, "N/A")

    def get_prediction(self, track_id: int) -> Optional[Dict[str, float]]:
        """Retourne les probabilitÃ©s complÃ¨tes pour un track_id."""
        return self._last_predictions.get(track_id, None)

    def check_alerts(self) -> list:
        """
        VÃ©rifie si des actions d'alerte sont dÃ©tectÃ©es.
        
        Returns:
            Liste de (track_id, action, confidence)
        """
        alerts = []
        for tid, preds in self._last_predictions.items():
            for action in ALERT_ACTIONS:
                if action in preds and preds[action] > 0.7:  # Seuil d'alerte
                    alerts.append((tid, action, preds[action]))
        return alerts

    def cleanup_lost_ids(self, active_ids: set):
        """Nettoie les buffers et prÃ©dictions des IDs perdus."""
        self.buffer.cleanup_lost_ids(active_ids)
        lost = set(self._last_predictions.keys()) - active_ids
        for tid in lost:
            del self._last_predictions[tid]
            if tid in self._last_actions:
                del self._last_actions[tid]

    def get_stats(self) -> dict:
        """Statistiques du classificateur."""
        return {
            "device": str(self.device),
            "buffer_stats": self.buffer.get_stats(),
            "active_predictions": len(self._last_predictions),
            "current_actions": self._last_actions.copy()
        }
```

**âœ… CritÃ¨re de validation 4.5** :
```python
python -c "
import numpy as np
from src.behavior.action_classifier import ActionClassifier

classifier = ActionClassifier(device='cuda')
print(f'âœ… ActionClassifier initialisÃ© sur {classifier.device}')

# Simuler 35 frames pour 2 personnes
for frame in range(35):
    kpts1 = np.random.randn(17, 2).astype(np.float32) * 50
    kpts2 = np.random.randn(17, 2).astype(np.float32) * 50
    classifier.update(1, kpts1)
    classifier.update(2, kpts2)

# InfÃ©rence
preds = classifier.classify(frame_count=35)
print(f'âœ… PrÃ©dictions pour {len(preds)} personne(s)')
for tid, pred in preds.items():
    action = classifier.get_action(tid)
    print(f'  ID:{tid} â†’ {action} ({pred[action]:.1%})')

# VÃ©rifier les alertes
alerts = classifier.check_alerts()
print(f'âœ… Alertes dÃ©tectÃ©es : {len(alerts)}')
print(f'  Stats : {classifier.get_stats()}')
"
```

---

### 4.6 â€” CrÃ©er le DÃ©tecteur de Maraudage (`src/behavior/loitering_detector.py`)

> [!NOTE]
> Le ST-GCN analyse le **mouvement** mais pas le **temps passÃ© Ã  un endroit**. Le dÃ©tecteur de maraudage complÃ¨te le ST-GCN avec une rÃ¨gle spatiale basÃ©e sur un polygone.

**Actions :**

CrÃ©er `src/behavior/loitering_detector.py` :

```python
"""
DÃ©tecteur de maraudage (loitering).
VÃ©rifie si une personne reste trop longtemps dans une zone dÃ©finie
par un polygone.

Le ST-GCN ne gÃ¨re pas le temps â†’ cette rÃ¨gle spatiale le complÃ¨te.
"""
import time
import numpy as np
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import cv2
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))
from src.config import LOITERING_TIMEOUT


class LoiteringDetector:
    """
    DÃ©tecte le maraudage basÃ© sur le temps passÃ© dans une zone.
    
    Algorithme :
    1. DÃ©finir un (ou plusieurs) polygone(s) de surveillance
    2. Pour chaque personne, vÃ©rifier si son centre est dans le polygone
    3. Si elle y reste plus de LOITERING_TIMEOUT â†’ alerte MARAUDAGE
    """

    def __init__(self, timeout: float = LOITERING_TIMEOUT):
        """
        Args:
            timeout: Temps en secondes avant alerte de maraudage
        """
        self.timeout = timeout
        self.zones: List[np.ndarray] = []  # Liste de polygones
        
        # Suivi du temps par personne par zone
        # {track_id: {"zone_idx": zone_index, "enter_time": float}}
        self._tracking: Dict[int, Dict] = {}

    def add_zone(self, polygon: list):
        """
        Ajoute une zone de surveillance.
        
        Args:
            polygon: Liste de points [(x1,y1), (x2,y2), ...] 
                     dÃ©finissant le polygone
        """
        poly = np.array(polygon, dtype=np.int32)
        self.zones.append(poly)
        print(f"[LOITERING] Zone ajoutÃ©e ({len(poly)} points)")

    def set_default_zones(self, frame_width: int, frame_height: int):
        """
        CrÃ©e une zone par dÃ©faut couvrant le centre de l'image.
        Utile pour les tests. En production, dÃ©finir les zones manuellement.
        """
        # Zone centrale (60% de l'image)
        margin_x = int(frame_width * 0.2)
        margin_y = int(frame_height * 0.2)
        default_zone = [
            (margin_x, margin_y),
            (frame_width - margin_x, margin_y),
            (frame_width - margin_x, frame_height - margin_y),
            (margin_x, frame_height - margin_y),
        ]
        self.add_zone(default_zone)

    def is_in_zone(self, point: tuple, zone_idx: int = 0) -> bool:
        """VÃ©rifie si un point est dans la zone spÃ©cifiÃ©e."""
        if zone_idx >= len(self.zones):
            return False
        result = cv2.pointPolygonTest(self.zones[zone_idx], point, False)
        return result >= 0

    def update(self, track_id: int, center: tuple) -> Optional[Tuple[str, float]]:
        """
        Met Ã  jour le suivi de maraudage pour une personne.
        
        Args:
            track_id: ID de suivi
            center: (x, y) centre de la personne
            
        Returns:
            ("MARAUDAGE", durÃ©e_en_secondes) si alerte, ou None
        """
        now = time.time()
        
        # VÃ©rifier chaque zone
        in_any_zone = False
        for zone_idx, zone in enumerate(self.zones):
            if self.is_in_zone(center, zone_idx):
                in_any_zone = True
                
                if track_id not in self._tracking:
                    self._tracking[track_id] = {
                        "zone_idx": zone_idx,
                        "enter_time": now
                    }
                else:
                    duration = now - self._tracking[track_id]["enter_time"]
                    if duration >= self.timeout:
                        return ("MARAUDAGE", duration)
                break
        
        # Si pas dans une zone, rÃ©initialiser le compteur
        if not in_any_zone and track_id in self._tracking:
            del self._tracking[track_id]
        
        return None

    def cleanup_lost_ids(self, active_ids: set):
        """Nettoie les IDs perdus."""
        lost = set(self._tracking.keys()) - active_ids
        for tid in lost:
            del self._tracking[tid]

    def draw_zones(self, frame: np.ndarray) -> np.ndarray:
        """Dessine les zones de surveillance sur la frame."""
        overlay = frame.copy()
        for zone in self.zones:
            cv2.polylines(overlay, [zone], True, (0, 200, 255), 2)
            # Zone semi-transparente
            cv2.fillPoly(overlay, [zone], (0, 200, 255))
        cv2.addWeighted(overlay, 0.15, frame, 0.85, 0, frame)
        return frame

    def get_stats(self) -> dict:
        """Statistiques du dÃ©tecteur."""
        return {
            "num_zones": len(self.zones),
            "tracked_persons": len(self._tracking),
            "durations": {
                tid: f"{time.time() - info['enter_time']:.0f}s"
                for tid, info in self._tracking.items()
            }
        }
```

**âœ… CritÃ¨re de validation 4.6** :
```python
python -c "
import numpy as np
from src.behavior.loitering_detector import LoiteringDetector

detector = LoiteringDetector(timeout=5)  # 5s pour le test
detector.set_default_zones(1920, 1080)

print(f'âœ… LoiteringDetector initialisÃ©')
print(f'  Zones : {detector.get_stats()[\"num_zones\"]}')

# Test point dans la zone
assert detector.is_in_zone((960, 540), 0), 'Le centre devrait Ãªtre dans la zone'
assert not detector.is_in_zone((10, 10), 0), 'Le coin devrait Ãªtre hors zone'
print('âœ… DÃ©tection de zone OK')

# Test maraudage (simulation rapide)
import time
result = detector.update(1, (960, 540))
assert result is None, 'Pas de maraudage immÃ©diat'
time.sleep(6)  # Attendre plus que le timeout
result = detector.update(1, (960, 540))
assert result is not None
print(f'âœ… Maraudage dÃ©tectÃ© : {result}')
"
```

---

### 4.7 â€” CrÃ©er le Test Complet ST-GCN

**Actions :**

CrÃ©er `tests/test_stgcn.py` :

```python
"""
Test complet du systÃ¨me ST-GCN intÃ©grÃ© au pipeline.
"""
import cv2
import time
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent))
from src.pipeline.detector import PoseDetector
from src.behavior.action_classifier import ActionClassifier
from src.behavior.loitering_detector import LoiteringDetector
from src.utils.drawing import draw_detections, draw_fps, draw_alert


def test_stgcn_pipeline(source=0):
    """Test le pipeline complet avec ST-GCN et dÃ©tection de maraudage."""
    
    print("[TEST ST-GCN] Initialisation...")
    detector = PoseDetector()
    classifier = ActionClassifier(device="cuda")
    loitering = LoiteringDetector(timeout=30)  # 30s pour le test
    
    cap = cv2.VideoCapture(source)
    if not cap.isOpened():
        print(f"âŒ Impossible d'ouvrir : {source}")
        return False
    
    # Zones de maraudage
    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    loitering.set_default_zones(w, h)
    
    fps_start = time.time()
    fps_counter = 0
    current_fps = 0.0
    
    print("[TEST ST-GCN] DÃ©marrage ('q' pour quitter)")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            if isinstance(source, str):
                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                continue
            break
        
        # 1. DÃ©tection
        detections = detector.detect(frame)
        active_ids = {d.track_id for d in detections}
        
        # 2. Mise Ã  jour des buffers + classification
        for det in detections:
            classifier.update(det.track_id, det.keypoints_xy)
        
        predictions = classifier.classify(detector.frame_count)
        
        # 3. Appliquer les actions aux dÃ©tections
        for det in detections:
            det.action = classifier.get_action(det.track_id)
            
            # 4. VÃ©rifier le maraudage
            loiter_result = loitering.update(det.track_id, det.center)
            if loiter_result:
                det.action = f"MARAUDAGE ({loiter_result[1]:.0f}s)"
        
        # 5. VÃ©rifier les alertes ST-GCN
        alerts = classifier.check_alerts()
        
        # 6. Affichage
        frame = loitering.draw_zones(frame)
        annotated = draw_detections(frame, detections)
        annotated = draw_fps(annotated, current_fps)
        
        for alert_tid, alert_action, alert_conf in alerts:
            draw_alert(annotated, f"{alert_action} (ID:{alert_tid} - {alert_conf:.0%})")
        
        # Nettoyage
        classifier.cleanup_lost_ids(active_ids)
        loitering.cleanup_lost_ids(active_ids)
        
        # FPS
        fps_counter += 1
        if time.time() - fps_start >= 1.0:
            current_fps = fps_counter / (time.time() - fps_start)
            fps_counter = 0
            fps_start = time.time()
        
        cv2.imshow("CCTV AI - Test ST-GCN", annotated)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
    
    print(f"\n[TEST ST-GCN] Stats classificateur : {classifier.get_stats()}")
    print(f"[TEST ST-GCN] Stats maraudage : {loitering.get_stats()}")
    return True


if __name__ == "__main__":
    source = sys.argv[1] if len(sys.argv) > 1 else 0
    success = test_stgcn_pipeline(source)
    print(f"\n{'âœ… TEST RÃ‰USSI' if success else 'âŒ TEST Ã‰CHOUÃ‰'}")
```

**âœ… CritÃ¨re de validation 4.7** :
```powershell
python tests/test_stgcn.py

# DOIT :
# - Afficher les boÃ®tes + squelettes + IDs (Ã©tape 1)
# - Afficher l'action prÃ©dite pour chaque personne (ex: "marcher", "immobile")
# - Afficher les zones de surveillance en orange semi-transparent
# - Afficher une alerte rouge si une action dangereuse est dÃ©tectÃ©e
# - Les actions changent dynamiquement avec le mouvement de la personne
```

---

## ğŸ“ Note sur l'EntraÃ®nement du ModÃ¨le

> [!WARNING]
> Le modÃ¨le crÃ©Ã© dans cette Ã©tape a des **poids alÃ©atoires**. Pour un systÃ¨me de production, deux options :
> 
> **Option 1 â€” Transfer Learning (RecommandÃ©)** :
> 1. TÃ©lÃ©charger un modÃ¨le ST-GCN prÃ©-entraÃ®nÃ© sur NTU-RGB+D (60 classes)
> 2. Remplacer la derniÃ¨re couche FC pour vos 8 classes
> 3. Fine-tuner sur vos propres donnÃ©es
> 
> **Option 2 â€” Utiliser un modÃ¨le prÃ©-entraÃ®nÃ© tel quel** :
> 1. TÃ©lÃ©charger depuis [pyskl](https://github.com/kennymckormick/pyskl) ou [st-gcn](https://github.com/yysijie/st-gcn)
> 2. Adapter le mapping des classes de sortie
> 
> L'entraÃ®nement n'est PAS couvert dans ces Ã©tapes mais peut Ãªtre ajoutÃ© comme Phase 6.

---

## âœ… Checklist de Validation Finale â€” Ã‰tape 4

| # | CritÃ¨re | Commande/Action | Status |
|---|---------|-----------------|--------|
| 4.2 | ModÃ¨le ST-GCN disponible | DÃ©pÃ´t clonÃ© ou pyskl installÃ© | â¬œ |
| 4.3 | `skeleton_buffer.py` testÃ© | Buffer (2,30,17) crÃ©Ã© correctement | â¬œ |
| 4.4 | `model.py` (ST-GCN) testÃ© | Forward pass (1,2,30,17)â†’(1,8) | â¬œ |
| 4.5 | `action_classifier.py` testÃ© | PrÃ©dictions pour N personnes | â¬œ |
| 4.6 | `loitering_detector.py` testÃ© | Maraudage dÃ©tectÃ© aprÃ¨s timeout | â¬œ |
| 4.7 | Test pipeline complet | VidÃ©o avec actions + zones + alertes | â¬œ |

**VÃ©rifications fonctionnelles obligatoires :**
- [ ] Le buffer se remplit correctement (30 frames = 1s Ã  30fps)
- [ ] Le ST-GCN produit des probabilitÃ©s valides (somme â‰ˆ 1.0)
- [ ] Les actions sont affichÃ©es Ã  cÃ´tÃ© de chaque personne
- [ ] Les zones de maraudage sont visibles et fonctionnelles
- [ ] Les alertes de maraudage se dÃ©clenchent aprÃ¨s le timeout
- [ ] Le systÃ¨me ne crash pas quand des personnes entrent/sortent du champ

> [!CAUTION]
> **Cette Ã©tape est la plus complexe.** Testez chaque sous-module individuellement avant le test intÃ©grÃ©. Si le ST-GCN ne fonctionne pas, le reste du pipeline (Ã‰tapes 1-3) reste valide.

---

**â¬…ï¸ Ã‰tape prÃ©cÃ©dente : [etape_3.md](etape_3.md)**
**â¡ï¸ Ã‰tape suivante : [etape_5.md](etape_5.md) â€” Optimisation RTX 3080 Ti (Pipeline Asynchrone)**
